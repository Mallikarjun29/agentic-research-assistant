# Jan-v1 Deep Research Assistant

Build a local, privacy-preserving research workflow that combines Jan-v1's agentic reasoning with a Streamlit front-end and Hugging Face Transformers inference tuned for Tesla T4–A10 GPUs.

## What You Get
- **Agent-ready reasoning**: Jan-v1 (Lucy + Qwen3-4B-thinking) orchestrates query planning, scout-note generation, and sectioned reporting.
- **GPU-accelerated deployment**: Load the `janhq/Jan-v1-4B` transformer weights directly from Hugging Face with FP16 on NVIDIA T4/A10 class GPUs.
- **Self-contained research loop**: Instead of Serper, the model drafts its own scout briefs per query and then synthesizes a final report from those notes.
- **Streamlit demo app**: Configure topic, depth, focus, timeframe, and report format with real-time progress bars and export buttons.

## Repository Layout
```
app.py                      # Streamlit UI and user workflow
jan_assistant/
    __init__.py             # Package export surface
    assistant.py            # DeepResearchAssistant orchestrator
    config.py               # ResearchConfig dataclass
    utils.py                # Formatting helpers
requirements.txt            # Python dependencies
```

## Prerequisites
- Python 3.10+
- NVIDIA GPU with ≥16 GB VRAM (Tesla T4, L4, A10, etc.) and CUDA 12 capable drivers
- Sufficient disk bandwidth to stream ~8 GB of model weights from Hugging Face
- Optional but recommended: `HF_TOKEN` environment variable for higher-rate downloads

## 1. Install Dependencies
```bash
python -m venv .venv
source .venv/bin/activate
# Install a CUDA build of PyTorch first (example shown for CUDA 12.1)
pip install torch --index-url https://download.pytorch.org/whl/cu121
pip install -r requirements.txt
```

## 2. (Optional) Pre-download the model from Hugging Face
The first run will automatically download `janhq/Jan-v1-4B`. To prefetch or use offline clusters:
```bash
huggingface-cli download janhq/Jan-v1-4B --local-dir ./models/jan-v1-4b
```
Set `HF_HOME` if you want to control the cache location.

## 3. Run the Streamlit Demo
```bash
streamlit run app.py
```
Open the provided URL and:
1. Enter a research topic (e.g., *Impact of open-source LLM stacks on biotech*).
2. Choose depth, focus (general, academic, business, technical, policy), timeframe, and desired report format.
3. Click **Start Deep Research** to trigger query generation, LLM scout briefs, and Jan-v1 synthesis.
4. Review the structured report, inspect the autogenerated sources, and export in TXT or JSON.

## How It Works
1. **Configuration** (`jan_assistant/config.py`): Holds the Hugging Face model ID, HF token, sampling params, and device preferences (e.g., `device_map="auto"`, FP16).
2. **Assistant Orchestrator** (`jan_assistant/assistant.py`):
  - Loads `janhq/Jan-v1-4B` via `AutoModelForCausalLM.from_pretrained` using FP16 for T4/A10 GPUs.
  - Generates deterministic search queries based on topic depth and focus.
  - Uses the model itself to draft scout briefs for each query, simulating a research crawl without Serper.
  - Synthesizes the final report from those notes, enforcing `<final>...</final>` tags and strict section ordering.
3. **Utilities** (`jan_assistant/utils.py`): Provide report section templates and regex-based `<final>` extraction/cleanup.
4. **UI Layer** (`app.py`): Streamlit manages session-scoped model objects, exposes controls, tracks progress, and offers export buttons.

## Troubleshooting
- **`Model not loaded.`** Confirm PyTorch sees your GPU (`python -c "import torch; print(torch.cuda.is_available())"`) and that you installed the CUDA wheel. Set `HF_TOKEN` if downloads fail.
- **OOM errors**: Reduce `max_tokens` in `ResearchConfig`, set `device_map="auto"`, or run on a larger GPU (A10, L40, etc.).
- **Slow first run**: Hugging Face caches weights under `~/.cache/huggingface`; relocate via `HF_HOME` if needed.

## Next Steps
- Plug in additional tools (e.g., vector retrieval, PDF parsing) inside `DeepResearchAssistant`.
- Extend the Streamlit UI with run histories or multi-report queues.
- Package the assistant as a CLI for batch research jobs.

Enjoy exploring Jan-v1's agentic reasoning on your GPU!
